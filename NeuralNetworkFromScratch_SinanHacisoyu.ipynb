{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network from Scratch\n",
    "\n",
    "https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract Base Class : Layer\n",
    "The abstract class Layer, which all other layers will inherit from, handles simple properties which are an input, an output, and both a forward and backward methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the abstract class above, backward_propagation function has an extra parameter, learning_rate, which is controlling the amount of learning/updating parameters using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "Suppose we have a matrix containing the derivative of the error with respect to that layer’s output: $\\frac{\\partial E}{\\partial Y}$\n",
    "\n",
    "We need :\n",
    "- The derivative of the error with respect to the parameters ($\\frac{\\partial E}{\\partial W}$, $\\frac{\\partial E}{\\partial B}$)\n",
    "- The derivative of the error with respect to the input ($\\frac{\\partial E}{\\partial X}$)\n",
    "\n",
    "Let's calculate $\\frac{\\partial E}{\\partial W}$. This matrix should be the same size as $W$ itself : \n",
    "\n",
    "$i x j$ where $i$ is the number of input neurons and $j$ the number of output neurons. We need one gradient for every weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from layer import Layer\n",
    "import numpy as np\n",
    "\n",
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of edges that connects to neurons in next layer\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Layer\n",
    "All the calculation we did until now were completely linear, may not learn well. We need to add non-linearity to the model by applying non-linear functions to the output of some layers.\n",
    "\n",
    "Now we need to redo the whole process for this new type of layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from layer import Layer\n",
    "\n",
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write some activation functions and their derivatives in a separate file. These will be used later to create an ActivationLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "Until now, for a given layer, we supposed that ∂E/∂Y was given (by the next layer). But what happens to the last layer? How does it get ∂E/∂Y? We simply give it manually, and it depends on how we define the error.\n",
    "The error of the network, which measures how good or bad the network did for a given input data, is defined by you. \n",
    "\n",
    "There are many ways to define the error, and one of the most known is called MSE — Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Class\n",
    "Almost done ! We are going to make a Network class to create neural networks very easily using the building blocks we have prepared so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a function for calculating softmax for a list of numbers\n",
    "from numpy import exp\n",
    " \n",
    "# calculate the softmax of a vector\n",
    "def softmax(vector):\n",
    "    e = exp(vector)\n",
    "    return e / e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "        \n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network \n",
    "    \n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        '''\n",
    "        Fit function does the training. \n",
    "        Training data is passed 1-by-1 through the network layers during forward propagation.\n",
    "        Loss (error) is calculated for each input and back propagation is performed via partial \n",
    "        derivatives on each layer.\n",
    "        '''\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Neural Networks\n",
    "Finally ! We can use our class to create a neural network with as many layers as we want ! We are going to build two neural networks : a simple XOR and a MNIST solver.\n",
    "\n",
    "\n",
    "### Solve XOR\n",
    "Starting with XOR is always important as it’s a simple way to tell if the network is learning anything at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.296054\n",
      "epoch 2/1000   error=0.295202\n",
      "epoch 3/1000   error=0.294748\n",
      "epoch 4/1000   error=0.294448\n",
      "epoch 5/1000   error=0.294217\n",
      "epoch 6/1000   error=0.294020\n",
      "epoch 7/1000   error=0.293839\n",
      "epoch 8/1000   error=0.293669\n",
      "epoch 9/1000   error=0.293504\n",
      "epoch 10/1000   error=0.293344\n",
      "epoch 11/1000   error=0.293187\n",
      "epoch 12/1000   error=0.293032\n",
      "epoch 13/1000   error=0.292880\n",
      "epoch 14/1000   error=0.292729\n",
      "epoch 15/1000   error=0.292580\n",
      "epoch 16/1000   error=0.292433\n",
      "epoch 17/1000   error=0.292288\n",
      "epoch 18/1000   error=0.292143\n",
      "epoch 19/1000   error=0.292000\n",
      "epoch 20/1000   error=0.291859\n",
      "epoch 21/1000   error=0.291718\n",
      "epoch 22/1000   error=0.291578\n",
      "epoch 23/1000   error=0.291440\n",
      "epoch 24/1000   error=0.291302\n",
      "epoch 25/1000   error=0.291165\n",
      "epoch 26/1000   error=0.291028\n",
      "epoch 27/1000   error=0.290892\n",
      "epoch 28/1000   error=0.290757\n",
      "epoch 29/1000   error=0.290623\n",
      "epoch 30/1000   error=0.290488\n",
      "epoch 31/1000   error=0.290355\n",
      "epoch 32/1000   error=0.290221\n",
      "epoch 33/1000   error=0.290089\n",
      "epoch 34/1000   error=0.289956\n",
      "epoch 35/1000   error=0.289824\n",
      "epoch 36/1000   error=0.289692\n",
      "epoch 37/1000   error=0.289561\n",
      "epoch 38/1000   error=0.289430\n",
      "epoch 39/1000   error=0.289299\n",
      "epoch 40/1000   error=0.289169\n",
      "epoch 41/1000   error=0.289039\n",
      "epoch 42/1000   error=0.288909\n",
      "epoch 43/1000   error=0.288780\n",
      "epoch 44/1000   error=0.288651\n",
      "epoch 45/1000   error=0.288523\n",
      "epoch 46/1000   error=0.288395\n",
      "epoch 47/1000   error=0.288268\n",
      "epoch 48/1000   error=0.288142\n",
      "epoch 49/1000   error=0.288016\n",
      "epoch 50/1000   error=0.287891\n",
      "epoch 51/1000   error=0.287766\n",
      "epoch 52/1000   error=0.287643\n",
      "epoch 53/1000   error=0.287520\n",
      "epoch 54/1000   error=0.287398\n",
      "epoch 55/1000   error=0.287277\n",
      "epoch 56/1000   error=0.287156\n",
      "epoch 57/1000   error=0.287037\n",
      "epoch 58/1000   error=0.286919\n",
      "epoch 59/1000   error=0.286802\n",
      "epoch 60/1000   error=0.286686\n",
      "epoch 61/1000   error=0.286571\n",
      "epoch 62/1000   error=0.286458\n",
      "epoch 63/1000   error=0.286345\n",
      "epoch 64/1000   error=0.286234\n",
      "epoch 65/1000   error=0.286124\n",
      "epoch 66/1000   error=0.286016\n",
      "epoch 67/1000   error=0.285908\n",
      "epoch 68/1000   error=0.285802\n",
      "epoch 69/1000   error=0.285698\n",
      "epoch 70/1000   error=0.285594\n",
      "epoch 71/1000   error=0.285493\n",
      "epoch 72/1000   error=0.285392\n",
      "epoch 73/1000   error=0.285293\n",
      "epoch 74/1000   error=0.285195\n",
      "epoch 75/1000   error=0.285098\n",
      "epoch 76/1000   error=0.285003\n",
      "epoch 77/1000   error=0.284909\n",
      "epoch 78/1000   error=0.284817\n",
      "epoch 79/1000   error=0.284725\n",
      "epoch 80/1000   error=0.284635\n",
      "epoch 81/1000   error=0.284547\n",
      "epoch 82/1000   error=0.284459\n",
      "epoch 83/1000   error=0.284373\n",
      "epoch 84/1000   error=0.284288\n",
      "epoch 85/1000   error=0.284205\n",
      "epoch 86/1000   error=0.284122\n",
      "epoch 87/1000   error=0.284041\n",
      "epoch 88/1000   error=0.283961\n",
      "epoch 89/1000   error=0.283882\n",
      "epoch 90/1000   error=0.283804\n",
      "epoch 91/1000   error=0.283727\n",
      "epoch 92/1000   error=0.283651\n",
      "epoch 93/1000   error=0.283577\n",
      "epoch 94/1000   error=0.283503\n",
      "epoch 95/1000   error=0.283431\n",
      "epoch 96/1000   error=0.283359\n",
      "epoch 97/1000   error=0.283288\n",
      "epoch 98/1000   error=0.283219\n",
      "epoch 99/1000   error=0.283150\n",
      "epoch 100/1000   error=0.283083\n",
      "epoch 101/1000   error=0.283016\n",
      "epoch 102/1000   error=0.282951\n",
      "epoch 103/1000   error=0.282886\n",
      "epoch 104/1000   error=0.282822\n",
      "epoch 105/1000   error=0.282759\n",
      "epoch 106/1000   error=0.282697\n",
      "epoch 107/1000   error=0.282636\n",
      "epoch 108/1000   error=0.282576\n",
      "epoch 109/1000   error=0.282516\n",
      "epoch 110/1000   error=0.282458\n",
      "epoch 111/1000   error=0.282401\n",
      "epoch 112/1000   error=0.282344\n",
      "epoch 113/1000   error=0.282288\n",
      "epoch 114/1000   error=0.282233\n",
      "epoch 115/1000   error=0.282179\n",
      "epoch 116/1000   error=0.282126\n",
      "epoch 117/1000   error=0.282074\n",
      "epoch 118/1000   error=0.282022\n",
      "epoch 119/1000   error=0.281972\n",
      "epoch 120/1000   error=0.281922\n",
      "epoch 121/1000   error=0.281873\n",
      "epoch 122/1000   error=0.281825\n",
      "epoch 123/1000   error=0.281778\n",
      "epoch 124/1000   error=0.281732\n",
      "epoch 125/1000   error=0.281686\n",
      "epoch 126/1000   error=0.281641\n",
      "epoch 127/1000   error=0.281598\n",
      "epoch 128/1000   error=0.281555\n",
      "epoch 129/1000   error=0.281512\n",
      "epoch 130/1000   error=0.281471\n",
      "epoch 131/1000   error=0.281431\n",
      "epoch 132/1000   error=0.281391\n",
      "epoch 133/1000   error=0.281352\n",
      "epoch 134/1000   error=0.281314\n",
      "epoch 135/1000   error=0.281276\n",
      "epoch 136/1000   error=0.281240\n",
      "epoch 137/1000   error=0.281204\n",
      "epoch 138/1000   error=0.281169\n",
      "epoch 139/1000   error=0.281134\n",
      "epoch 140/1000   error=0.281101\n",
      "epoch 141/1000   error=0.281068\n",
      "epoch 142/1000   error=0.281036\n",
      "epoch 143/1000   error=0.281004\n",
      "epoch 144/1000   error=0.280973\n",
      "epoch 145/1000   error=0.280943\n",
      "epoch 146/1000   error=0.280914\n",
      "epoch 147/1000   error=0.280885\n",
      "epoch 148/1000   error=0.280857\n",
      "epoch 149/1000   error=0.280829\n",
      "epoch 150/1000   error=0.280803\n",
      "epoch 151/1000   error=0.280776\n",
      "epoch 152/1000   error=0.280750\n",
      "epoch 153/1000   error=0.280725\n",
      "epoch 154/1000   error=0.280701\n",
      "epoch 155/1000   error=0.280677\n",
      "epoch 156/1000   error=0.280653\n",
      "epoch 157/1000   error=0.280630\n",
      "epoch 158/1000   error=0.280608\n",
      "epoch 159/1000   error=0.280586\n",
      "epoch 160/1000   error=0.280564\n",
      "epoch 161/1000   error=0.280543\n",
      "epoch 162/1000   error=0.280523\n",
      "epoch 163/1000   error=0.280502\n",
      "epoch 164/1000   error=0.280483\n",
      "epoch 165/1000   error=0.280463\n",
      "epoch 166/1000   error=0.280444\n",
      "epoch 167/1000   error=0.280426\n",
      "epoch 168/1000   error=0.280408\n",
      "epoch 169/1000   error=0.280390\n",
      "epoch 170/1000   error=0.280372\n",
      "epoch 171/1000   error=0.280355\n",
      "epoch 172/1000   error=0.280339\n",
      "epoch 173/1000   error=0.280322\n",
      "epoch 174/1000   error=0.280306\n",
      "epoch 175/1000   error=0.280290\n",
      "epoch 176/1000   error=0.280275\n",
      "epoch 177/1000   error=0.280259\n",
      "epoch 178/1000   error=0.280244\n",
      "epoch 179/1000   error=0.280230\n",
      "epoch 180/1000   error=0.280215\n",
      "epoch 181/1000   error=0.280201\n",
      "epoch 182/1000   error=0.280187\n",
      "epoch 183/1000   error=0.280173\n",
      "epoch 184/1000   error=0.280159\n",
      "epoch 185/1000   error=0.280146\n",
      "epoch 186/1000   error=0.280133\n",
      "epoch 187/1000   error=0.280120\n",
      "epoch 188/1000   error=0.280107\n",
      "epoch 189/1000   error=0.280094\n",
      "epoch 190/1000   error=0.280082\n",
      "epoch 191/1000   error=0.280069\n",
      "epoch 192/1000   error=0.280057\n",
      "epoch 193/1000   error=0.280045\n",
      "epoch 194/1000   error=0.280033\n",
      "epoch 195/1000   error=0.280022\n",
      "epoch 196/1000   error=0.280010\n",
      "epoch 197/1000   error=0.279999\n",
      "epoch 198/1000   error=0.279987\n",
      "epoch 199/1000   error=0.279976\n",
      "epoch 200/1000   error=0.279965\n",
      "epoch 201/1000   error=0.279954\n",
      "epoch 202/1000   error=0.279943\n",
      "epoch 203/1000   error=0.279932\n",
      "epoch 204/1000   error=0.279921\n",
      "epoch 205/1000   error=0.279911\n",
      "epoch 206/1000   error=0.279900\n",
      "epoch 207/1000   error=0.279890\n",
      "epoch 208/1000   error=0.279880\n",
      "epoch 209/1000   error=0.279869\n",
      "epoch 210/1000   error=0.279859\n",
      "epoch 211/1000   error=0.279849\n",
      "epoch 212/1000   error=0.279839\n",
      "epoch 213/1000   error=0.279829\n",
      "epoch 214/1000   error=0.279819\n",
      "epoch 215/1000   error=0.279809\n",
      "epoch 216/1000   error=0.279799\n",
      "epoch 217/1000   error=0.279790\n",
      "epoch 218/1000   error=0.279780\n",
      "epoch 219/1000   error=0.279770\n",
      "epoch 220/1000   error=0.279761\n",
      "epoch 221/1000   error=0.279751\n",
      "epoch 222/1000   error=0.279742\n",
      "epoch 223/1000   error=0.279733\n",
      "epoch 224/1000   error=0.279723\n",
      "epoch 225/1000   error=0.279714\n",
      "epoch 226/1000   error=0.279705\n",
      "epoch 227/1000   error=0.279696\n",
      "epoch 228/1000   error=0.279687\n",
      "epoch 229/1000   error=0.279677\n",
      "epoch 230/1000   error=0.279668\n",
      "epoch 231/1000   error=0.279659\n",
      "epoch 232/1000   error=0.279650\n",
      "epoch 233/1000   error=0.279642\n",
      "epoch 234/1000   error=0.279633\n",
      "epoch 235/1000   error=0.279624\n",
      "epoch 236/1000   error=0.279615\n",
      "epoch 237/1000   error=0.279606\n",
      "epoch 238/1000   error=0.279598\n",
      "epoch 239/1000   error=0.279589\n",
      "epoch 240/1000   error=0.279580\n",
      "epoch 241/1000   error=0.279572\n",
      "epoch 242/1000   error=0.279563\n",
      "epoch 243/1000   error=0.279555\n",
      "epoch 244/1000   error=0.279546\n",
      "epoch 245/1000   error=0.279538\n",
      "epoch 246/1000   error=0.279530\n",
      "epoch 247/1000   error=0.279521\n",
      "epoch 248/1000   error=0.279513\n",
      "epoch 249/1000   error=0.279505\n",
      "epoch 250/1000   error=0.279497\n",
      "epoch 251/1000   error=0.279488\n",
      "epoch 252/1000   error=0.279480\n",
      "epoch 253/1000   error=0.279472\n",
      "epoch 254/1000   error=0.279464\n",
      "epoch 255/1000   error=0.279456\n",
      "epoch 256/1000   error=0.279448\n",
      "epoch 257/1000   error=0.279440\n",
      "epoch 258/1000   error=0.279432\n",
      "epoch 259/1000   error=0.279425\n",
      "epoch 260/1000   error=0.279417\n",
      "epoch 261/1000   error=0.279409\n",
      "epoch 262/1000   error=0.279401\n",
      "epoch 263/1000   error=0.279394\n",
      "epoch 264/1000   error=0.279386\n",
      "epoch 265/1000   error=0.279379\n",
      "epoch 266/1000   error=0.279371\n",
      "epoch 267/1000   error=0.279364\n",
      "epoch 268/1000   error=0.279356\n",
      "epoch 269/1000   error=0.279349\n",
      "epoch 270/1000   error=0.279341\n",
      "epoch 271/1000   error=0.279334\n",
      "epoch 272/1000   error=0.279327\n",
      "epoch 273/1000   error=0.279320\n",
      "epoch 274/1000   error=0.279313\n",
      "epoch 275/1000   error=0.279305\n",
      "epoch 276/1000   error=0.279298\n",
      "epoch 277/1000   error=0.279291\n",
      "epoch 278/1000   error=0.279284\n",
      "epoch 279/1000   error=0.279277\n",
      "epoch 280/1000   error=0.279270\n",
      "epoch 281/1000   error=0.279264\n",
      "epoch 282/1000   error=0.279257\n",
      "epoch 283/1000   error=0.279250\n",
      "epoch 284/1000   error=0.279243\n",
      "epoch 285/1000   error=0.279237\n",
      "epoch 286/1000   error=0.279230\n",
      "epoch 287/1000   error=0.279224\n",
      "epoch 288/1000   error=0.279217\n",
      "epoch 289/1000   error=0.279211\n",
      "epoch 290/1000   error=0.279204\n",
      "epoch 291/1000   error=0.279198\n",
      "epoch 292/1000   error=0.279191\n",
      "epoch 293/1000   error=0.279185\n",
      "epoch 294/1000   error=0.279179\n",
      "epoch 295/1000   error=0.279173\n",
      "epoch 296/1000   error=0.279167\n",
      "epoch 297/1000   error=0.279160\n",
      "epoch 298/1000   error=0.279154\n",
      "epoch 299/1000   error=0.279148\n",
      "epoch 300/1000   error=0.279142\n",
      "epoch 301/1000   error=0.279137\n",
      "epoch 302/1000   error=0.279131\n",
      "epoch 303/1000   error=0.279125\n",
      "epoch 304/1000   error=0.279119\n",
      "epoch 305/1000   error=0.279113\n",
      "epoch 306/1000   error=0.279108\n",
      "epoch 307/1000   error=0.279102\n",
      "epoch 308/1000   error=0.279097\n",
      "epoch 309/1000   error=0.279091\n",
      "epoch 310/1000   error=0.279086\n",
      "epoch 311/1000   error=0.279080\n",
      "epoch 312/1000   error=0.279075\n",
      "epoch 313/1000   error=0.279069\n",
      "epoch 314/1000   error=0.279064\n",
      "epoch 315/1000   error=0.279059\n",
      "epoch 316/1000   error=0.279054\n",
      "epoch 317/1000   error=0.279049\n",
      "epoch 318/1000   error=0.279043\n",
      "epoch 319/1000   error=0.279038\n",
      "epoch 320/1000   error=0.279033\n",
      "epoch 321/1000   error=0.279028\n",
      "epoch 322/1000   error=0.279023\n",
      "epoch 323/1000   error=0.279019\n",
      "epoch 324/1000   error=0.279014\n",
      "epoch 325/1000   error=0.279009\n",
      "epoch 326/1000   error=0.279004\n",
      "epoch 327/1000   error=0.278999\n",
      "epoch 328/1000   error=0.278995\n",
      "epoch 329/1000   error=0.278990\n",
      "epoch 330/1000   error=0.278986\n",
      "epoch 331/1000   error=0.278981\n",
      "epoch 332/1000   error=0.278977\n",
      "epoch 333/1000   error=0.278972\n",
      "epoch 334/1000   error=0.278968\n",
      "epoch 335/1000   error=0.278963\n",
      "epoch 336/1000   error=0.278959\n",
      "epoch 337/1000   error=0.278955\n",
      "epoch 338/1000   error=0.278951\n",
      "epoch 339/1000   error=0.278946\n",
      "epoch 340/1000   error=0.278942\n",
      "epoch 341/1000   error=0.278938\n",
      "epoch 342/1000   error=0.278934\n",
      "epoch 343/1000   error=0.278930\n",
      "epoch 344/1000   error=0.278926\n",
      "epoch 345/1000   error=0.278922\n",
      "epoch 346/1000   error=0.278918\n",
      "epoch 347/1000   error=0.278914\n",
      "epoch 348/1000   error=0.278910\n",
      "epoch 349/1000   error=0.278907\n",
      "epoch 350/1000   error=0.278903\n",
      "epoch 351/1000   error=0.278899\n",
      "epoch 352/1000   error=0.278895\n",
      "epoch 353/1000   error=0.278892\n",
      "epoch 354/1000   error=0.278888\n",
      "epoch 355/1000   error=0.278884\n",
      "epoch 356/1000   error=0.278881\n",
      "epoch 357/1000   error=0.278877\n",
      "epoch 358/1000   error=0.278874\n",
      "epoch 359/1000   error=0.278870\n",
      "epoch 360/1000   error=0.278867\n",
      "epoch 361/1000   error=0.278864\n",
      "epoch 362/1000   error=0.278860\n",
      "epoch 363/1000   error=0.278857\n",
      "epoch 364/1000   error=0.278854\n",
      "epoch 365/1000   error=0.278851\n",
      "epoch 366/1000   error=0.278847\n",
      "epoch 367/1000   error=0.278844\n",
      "epoch 368/1000   error=0.278841\n",
      "epoch 369/1000   error=0.278838\n",
      "epoch 370/1000   error=0.278835\n",
      "epoch 371/1000   error=0.278832\n",
      "epoch 372/1000   error=0.278829\n",
      "epoch 373/1000   error=0.278826\n",
      "epoch 374/1000   error=0.278823\n",
      "epoch 375/1000   error=0.278820\n",
      "epoch 376/1000   error=0.278817\n",
      "epoch 377/1000   error=0.278814\n",
      "epoch 378/1000   error=0.278811\n",
      "epoch 379/1000   error=0.278809\n",
      "epoch 380/1000   error=0.278806\n",
      "epoch 381/1000   error=0.278803\n",
      "epoch 382/1000   error=0.278800\n",
      "epoch 383/1000   error=0.278798\n",
      "epoch 384/1000   error=0.278795\n",
      "epoch 385/1000   error=0.278792\n",
      "epoch 386/1000   error=0.278790\n",
      "epoch 387/1000   error=0.278787\n",
      "epoch 388/1000   error=0.278785\n",
      "epoch 389/1000   error=0.278782\n",
      "epoch 390/1000   error=0.278780\n",
      "epoch 391/1000   error=0.278777\n",
      "epoch 392/1000   error=0.278775\n",
      "epoch 393/1000   error=0.278772\n",
      "epoch 394/1000   error=0.278770\n",
      "epoch 395/1000   error=0.278768\n",
      "epoch 396/1000   error=0.278765\n",
      "epoch 397/1000   error=0.278763\n",
      "epoch 398/1000   error=0.278761\n",
      "epoch 399/1000   error=0.278759\n",
      "epoch 400/1000   error=0.278756\n",
      "epoch 401/1000   error=0.278754\n",
      "epoch 402/1000   error=0.278752\n",
      "epoch 403/1000   error=0.278750\n",
      "epoch 404/1000   error=0.278748\n",
      "epoch 405/1000   error=0.278746\n",
      "epoch 406/1000   error=0.278743\n",
      "epoch 407/1000   error=0.278741\n",
      "epoch 408/1000   error=0.278739\n",
      "epoch 409/1000   error=0.278737\n",
      "epoch 410/1000   error=0.278735\n",
      "epoch 411/1000   error=0.278733\n",
      "epoch 412/1000   error=0.278731\n",
      "epoch 413/1000   error=0.278729\n",
      "epoch 414/1000   error=0.278727\n",
      "epoch 415/1000   error=0.278726\n",
      "epoch 416/1000   error=0.278724\n",
      "epoch 417/1000   error=0.278722\n",
      "epoch 418/1000   error=0.278720\n",
      "epoch 419/1000   error=0.278718\n",
      "epoch 420/1000   error=0.278716\n",
      "epoch 421/1000   error=0.278715\n",
      "epoch 422/1000   error=0.278713\n",
      "epoch 423/1000   error=0.278711\n",
      "epoch 424/1000   error=0.278709\n",
      "epoch 425/1000   error=0.278708\n",
      "epoch 426/1000   error=0.278706\n",
      "epoch 427/1000   error=0.278704\n",
      "epoch 428/1000   error=0.278703\n",
      "epoch 429/1000   error=0.278701\n",
      "epoch 430/1000   error=0.278699\n",
      "epoch 431/1000   error=0.278698\n",
      "epoch 432/1000   error=0.278696\n",
      "epoch 433/1000   error=0.278695\n",
      "epoch 434/1000   error=0.278693\n",
      "epoch 435/1000   error=0.278692\n",
      "epoch 436/1000   error=0.278690\n",
      "epoch 437/1000   error=0.278689\n",
      "epoch 438/1000   error=0.278687\n",
      "epoch 439/1000   error=0.278686\n",
      "epoch 440/1000   error=0.278684\n",
      "epoch 441/1000   error=0.278683\n",
      "epoch 442/1000   error=0.278681\n",
      "epoch 443/1000   error=0.278680\n",
      "epoch 444/1000   error=0.278679\n",
      "epoch 445/1000   error=0.278677\n",
      "epoch 446/1000   error=0.278676\n",
      "epoch 447/1000   error=0.278675\n",
      "epoch 448/1000   error=0.278673\n",
      "epoch 449/1000   error=0.278672\n",
      "epoch 450/1000   error=0.278671\n",
      "epoch 451/1000   error=0.278669\n",
      "epoch 452/1000   error=0.278668\n",
      "epoch 453/1000   error=0.278667\n",
      "epoch 454/1000   error=0.278666\n",
      "epoch 455/1000   error=0.278664\n",
      "epoch 456/1000   error=0.278663\n",
      "epoch 457/1000   error=0.278662\n",
      "epoch 458/1000   error=0.278661\n",
      "epoch 459/1000   error=0.278660\n",
      "epoch 460/1000   error=0.278659\n",
      "epoch 461/1000   error=0.278657\n",
      "epoch 462/1000   error=0.278656\n",
      "epoch 463/1000   error=0.278655\n",
      "epoch 464/1000   error=0.278654\n",
      "epoch 465/1000   error=0.278653\n",
      "epoch 466/1000   error=0.278652\n",
      "epoch 467/1000   error=0.278651\n",
      "epoch 468/1000   error=0.278650\n",
      "epoch 469/1000   error=0.278649\n",
      "epoch 470/1000   error=0.278648\n",
      "epoch 471/1000   error=0.278647\n",
      "epoch 472/1000   error=0.278646\n",
      "epoch 473/1000   error=0.278645\n",
      "epoch 474/1000   error=0.278644\n",
      "epoch 475/1000   error=0.278643\n",
      "epoch 476/1000   error=0.278642\n",
      "epoch 477/1000   error=0.278641\n",
      "epoch 478/1000   error=0.278640\n",
      "epoch 479/1000   error=0.278639\n",
      "epoch 480/1000   error=0.278638\n",
      "epoch 481/1000   error=0.278637\n",
      "epoch 482/1000   error=0.278636\n",
      "epoch 483/1000   error=0.278635\n",
      "epoch 484/1000   error=0.278634\n",
      "epoch 485/1000   error=0.278634\n",
      "epoch 486/1000   error=0.278633\n",
      "epoch 487/1000   error=0.278632\n",
      "epoch 488/1000   error=0.278631\n",
      "epoch 489/1000   error=0.278630\n",
      "epoch 490/1000   error=0.278629\n",
      "epoch 491/1000   error=0.278629\n",
      "epoch 492/1000   error=0.278628\n",
      "epoch 493/1000   error=0.278627\n",
      "epoch 494/1000   error=0.278626\n",
      "epoch 495/1000   error=0.278625\n",
      "epoch 496/1000   error=0.278625\n",
      "epoch 497/1000   error=0.278624\n",
      "epoch 498/1000   error=0.278623\n",
      "epoch 499/1000   error=0.278622\n",
      "epoch 500/1000   error=0.278622\n",
      "epoch 501/1000   error=0.278621\n",
      "epoch 502/1000   error=0.278620\n",
      "epoch 503/1000   error=0.278620\n",
      "epoch 504/1000   error=0.278619\n",
      "epoch 505/1000   error=0.278618\n",
      "epoch 506/1000   error=0.278618\n",
      "epoch 507/1000   error=0.278617\n",
      "epoch 508/1000   error=0.278616\n",
      "epoch 509/1000   error=0.278616\n",
      "epoch 510/1000   error=0.278615\n",
      "epoch 511/1000   error=0.278614\n",
      "epoch 512/1000   error=0.278614\n",
      "epoch 513/1000   error=0.278613\n",
      "epoch 514/1000   error=0.278612\n",
      "epoch 515/1000   error=0.278612\n",
      "epoch 516/1000   error=0.278611\n",
      "epoch 517/1000   error=0.278611\n",
      "epoch 518/1000   error=0.278610\n",
      "epoch 519/1000   error=0.278609\n",
      "epoch 520/1000   error=0.278609\n",
      "epoch 521/1000   error=0.278608\n",
      "epoch 522/1000   error=0.278608\n",
      "epoch 523/1000   error=0.278607\n",
      "epoch 524/1000   error=0.278607\n",
      "epoch 525/1000   error=0.278606\n",
      "epoch 526/1000   error=0.278606\n",
      "epoch 527/1000   error=0.278605\n",
      "epoch 528/1000   error=0.278605\n",
      "epoch 529/1000   error=0.278604\n",
      "epoch 530/1000   error=0.278604\n",
      "epoch 531/1000   error=0.278603\n",
      "epoch 532/1000   error=0.278603\n",
      "epoch 533/1000   error=0.278602\n",
      "epoch 534/1000   error=0.278602\n",
      "epoch 535/1000   error=0.278601\n",
      "epoch 536/1000   error=0.278601\n",
      "epoch 537/1000   error=0.278600\n",
      "epoch 538/1000   error=0.278600\n",
      "epoch 539/1000   error=0.278599\n",
      "epoch 540/1000   error=0.278599\n",
      "epoch 541/1000   error=0.278599\n",
      "epoch 542/1000   error=0.278598\n",
      "epoch 543/1000   error=0.278598\n",
      "epoch 544/1000   error=0.278597\n",
      "epoch 545/1000   error=0.278597\n",
      "epoch 546/1000   error=0.278596\n",
      "epoch 547/1000   error=0.278596\n",
      "epoch 548/1000   error=0.278596\n",
      "epoch 549/1000   error=0.278595\n",
      "epoch 550/1000   error=0.278595\n",
      "epoch 551/1000   error=0.278595\n",
      "epoch 552/1000   error=0.278594\n",
      "epoch 553/1000   error=0.278594\n",
      "epoch 554/1000   error=0.278593\n",
      "epoch 555/1000   error=0.278593\n",
      "epoch 556/1000   error=0.278593\n",
      "epoch 557/1000   error=0.278592\n",
      "epoch 558/1000   error=0.278592\n",
      "epoch 559/1000   error=0.278592\n",
      "epoch 560/1000   error=0.278591\n",
      "epoch 561/1000   error=0.278591\n",
      "epoch 562/1000   error=0.278591\n",
      "epoch 563/1000   error=0.278590\n",
      "epoch 564/1000   error=0.278590\n",
      "epoch 565/1000   error=0.278590\n",
      "epoch 566/1000   error=0.278590\n",
      "epoch 567/1000   error=0.278589\n",
      "epoch 568/1000   error=0.278589\n",
      "epoch 569/1000   error=0.278589\n",
      "epoch 570/1000   error=0.278588\n",
      "epoch 571/1000   error=0.278588\n",
      "epoch 572/1000   error=0.278588\n",
      "epoch 573/1000   error=0.278588\n",
      "epoch 574/1000   error=0.278587\n",
      "epoch 575/1000   error=0.278587\n",
      "epoch 576/1000   error=0.278587\n",
      "epoch 577/1000   error=0.278587\n",
      "epoch 578/1000   error=0.278586\n",
      "epoch 579/1000   error=0.278586\n",
      "epoch 580/1000   error=0.278586\n",
      "epoch 581/1000   error=0.278586\n",
      "epoch 582/1000   error=0.278585\n",
      "epoch 583/1000   error=0.278585\n",
      "epoch 584/1000   error=0.278585\n",
      "epoch 585/1000   error=0.278585\n",
      "epoch 586/1000   error=0.278585\n",
      "epoch 587/1000   error=0.278584\n",
      "epoch 588/1000   error=0.278584\n",
      "epoch 589/1000   error=0.278584\n",
      "epoch 590/1000   error=0.278584\n",
      "epoch 591/1000   error=0.278584\n",
      "epoch 592/1000   error=0.278583\n",
      "epoch 593/1000   error=0.278583\n",
      "epoch 594/1000   error=0.278583\n",
      "epoch 595/1000   error=0.278583\n",
      "epoch 596/1000   error=0.278583\n",
      "epoch 597/1000   error=0.278583\n",
      "epoch 598/1000   error=0.278582\n",
      "epoch 599/1000   error=0.278582\n",
      "epoch 600/1000   error=0.278582\n",
      "epoch 601/1000   error=0.278582\n",
      "epoch 602/1000   error=0.278582\n",
      "epoch 603/1000   error=0.278582\n",
      "epoch 604/1000   error=0.278581\n",
      "epoch 605/1000   error=0.278581\n",
      "epoch 606/1000   error=0.278581\n",
      "epoch 607/1000   error=0.278581\n",
      "epoch 608/1000   error=0.278581\n",
      "epoch 609/1000   error=0.278581\n",
      "epoch 610/1000   error=0.278581\n",
      "epoch 611/1000   error=0.278581\n",
      "epoch 612/1000   error=0.278580\n",
      "epoch 613/1000   error=0.278580\n",
      "epoch 614/1000   error=0.278580\n",
      "epoch 615/1000   error=0.278580\n",
      "epoch 616/1000   error=0.278580\n",
      "epoch 617/1000   error=0.278580\n",
      "epoch 618/1000   error=0.278580\n",
      "epoch 619/1000   error=0.278580\n",
      "epoch 620/1000   error=0.278580\n",
      "epoch 621/1000   error=0.278579\n",
      "epoch 622/1000   error=0.278579\n",
      "epoch 623/1000   error=0.278579\n",
      "epoch 624/1000   error=0.278579\n",
      "epoch 625/1000   error=0.278579\n",
      "epoch 626/1000   error=0.278579\n",
      "epoch 627/1000   error=0.278579\n",
      "epoch 628/1000   error=0.278579\n",
      "epoch 629/1000   error=0.278579\n",
      "epoch 630/1000   error=0.278579\n",
      "epoch 631/1000   error=0.278579\n",
      "epoch 632/1000   error=0.278579\n",
      "epoch 633/1000   error=0.278579\n",
      "epoch 634/1000   error=0.278579\n",
      "epoch 635/1000   error=0.278578\n",
      "epoch 636/1000   error=0.278578\n",
      "epoch 637/1000   error=0.278578\n",
      "epoch 638/1000   error=0.278578\n",
      "epoch 639/1000   error=0.278578\n",
      "epoch 640/1000   error=0.278578\n",
      "epoch 641/1000   error=0.278578\n",
      "epoch 642/1000   error=0.278578\n",
      "epoch 643/1000   error=0.278578\n",
      "epoch 644/1000   error=0.278578\n",
      "epoch 645/1000   error=0.278578\n",
      "epoch 646/1000   error=0.278578\n",
      "epoch 647/1000   error=0.278578\n",
      "epoch 648/1000   error=0.278578\n",
      "epoch 649/1000   error=0.278578\n",
      "epoch 650/1000   error=0.278578\n",
      "epoch 651/1000   error=0.278578\n",
      "epoch 652/1000   error=0.278578\n",
      "epoch 653/1000   error=0.278578\n",
      "epoch 654/1000   error=0.278578\n",
      "epoch 655/1000   error=0.278578\n",
      "epoch 656/1000   error=0.278578\n",
      "epoch 657/1000   error=0.278578\n",
      "epoch 658/1000   error=0.278578\n",
      "epoch 659/1000   error=0.278578\n",
      "epoch 660/1000   error=0.278578\n",
      "epoch 661/1000   error=0.278578\n",
      "epoch 662/1000   error=0.278578\n",
      "epoch 663/1000   error=0.278578\n",
      "epoch 664/1000   error=0.278578\n",
      "epoch 665/1000   error=0.278578\n",
      "epoch 666/1000   error=0.278578\n",
      "epoch 667/1000   error=0.278578\n",
      "epoch 668/1000   error=0.278578\n",
      "epoch 669/1000   error=0.278578\n",
      "epoch 670/1000   error=0.278578\n",
      "epoch 671/1000   error=0.278578\n",
      "epoch 672/1000   error=0.278578\n",
      "epoch 673/1000   error=0.278578\n",
      "epoch 674/1000   error=0.278578\n",
      "epoch 675/1000   error=0.278578\n",
      "epoch 676/1000   error=0.278578\n",
      "epoch 677/1000   error=0.278578\n",
      "epoch 678/1000   error=0.278578\n",
      "epoch 679/1000   error=0.278578\n",
      "epoch 680/1000   error=0.278578\n",
      "epoch 681/1000   error=0.278579\n",
      "epoch 682/1000   error=0.278579\n",
      "epoch 683/1000   error=0.278579\n",
      "epoch 684/1000   error=0.278579\n",
      "epoch 685/1000   error=0.278579\n",
      "epoch 686/1000   error=0.278579\n",
      "epoch 687/1000   error=0.278579\n",
      "epoch 688/1000   error=0.278579\n",
      "epoch 689/1000   error=0.278579\n",
      "epoch 690/1000   error=0.278579\n",
      "epoch 691/1000   error=0.278579\n",
      "epoch 692/1000   error=0.278579\n",
      "epoch 693/1000   error=0.278579\n",
      "epoch 694/1000   error=0.278579\n",
      "epoch 695/1000   error=0.278579\n",
      "epoch 696/1000   error=0.278579\n",
      "epoch 697/1000   error=0.278580\n",
      "epoch 698/1000   error=0.278580\n",
      "epoch 699/1000   error=0.278580\n",
      "epoch 700/1000   error=0.278580\n",
      "epoch 701/1000   error=0.278580\n",
      "epoch 702/1000   error=0.278580\n",
      "epoch 703/1000   error=0.278580\n",
      "epoch 704/1000   error=0.278580\n",
      "epoch 705/1000   error=0.278580\n",
      "epoch 706/1000   error=0.278580\n",
      "epoch 707/1000   error=0.278580\n",
      "epoch 708/1000   error=0.278580\n",
      "epoch 709/1000   error=0.278581\n",
      "epoch 710/1000   error=0.278581\n",
      "epoch 711/1000   error=0.278581\n",
      "epoch 712/1000   error=0.278581\n",
      "epoch 713/1000   error=0.278581\n",
      "epoch 714/1000   error=0.278581\n",
      "epoch 715/1000   error=0.278581\n",
      "epoch 716/1000   error=0.278581\n",
      "epoch 717/1000   error=0.278581\n",
      "epoch 718/1000   error=0.278581\n",
      "epoch 719/1000   error=0.278581\n",
      "epoch 720/1000   error=0.278582\n",
      "epoch 721/1000   error=0.278582\n",
      "epoch 722/1000   error=0.278582\n",
      "epoch 723/1000   error=0.278582\n",
      "epoch 724/1000   error=0.278582\n",
      "epoch 725/1000   error=0.278582\n",
      "epoch 726/1000   error=0.278582\n",
      "epoch 727/1000   error=0.278582\n",
      "epoch 728/1000   error=0.278582\n",
      "epoch 729/1000   error=0.278583\n",
      "epoch 730/1000   error=0.278583\n",
      "epoch 731/1000   error=0.278583\n",
      "epoch 732/1000   error=0.278583\n",
      "epoch 733/1000   error=0.278583\n",
      "epoch 734/1000   error=0.278583\n",
      "epoch 735/1000   error=0.278583\n",
      "epoch 736/1000   error=0.278583\n",
      "epoch 737/1000   error=0.278584\n",
      "epoch 738/1000   error=0.278584\n",
      "epoch 739/1000   error=0.278584\n",
      "epoch 740/1000   error=0.278584\n",
      "epoch 741/1000   error=0.278584\n",
      "epoch 742/1000   error=0.278584\n",
      "epoch 743/1000   error=0.278584\n",
      "epoch 744/1000   error=0.278584\n",
      "epoch 745/1000   error=0.278585\n",
      "epoch 746/1000   error=0.278585\n",
      "epoch 747/1000   error=0.278585\n",
      "epoch 748/1000   error=0.278585\n",
      "epoch 749/1000   error=0.278585\n",
      "epoch 750/1000   error=0.278585\n",
      "epoch 751/1000   error=0.278585\n",
      "epoch 752/1000   error=0.278586\n",
      "epoch 753/1000   error=0.278586\n",
      "epoch 754/1000   error=0.278586\n",
      "epoch 755/1000   error=0.278586\n",
      "epoch 756/1000   error=0.278586\n",
      "epoch 757/1000   error=0.278586\n",
      "epoch 758/1000   error=0.278586\n",
      "epoch 759/1000   error=0.278587\n",
      "epoch 760/1000   error=0.278587\n",
      "epoch 761/1000   error=0.278587\n",
      "epoch 762/1000   error=0.278587\n",
      "epoch 763/1000   error=0.278587\n",
      "epoch 764/1000   error=0.278587\n",
      "epoch 765/1000   error=0.278587\n",
      "epoch 766/1000   error=0.278588\n",
      "epoch 767/1000   error=0.278588\n",
      "epoch 768/1000   error=0.278588\n",
      "epoch 769/1000   error=0.278588\n",
      "epoch 770/1000   error=0.278588\n",
      "epoch 771/1000   error=0.278588\n",
      "epoch 772/1000   error=0.278589\n",
      "epoch 773/1000   error=0.278589\n",
      "epoch 774/1000   error=0.278589\n",
      "epoch 775/1000   error=0.278589\n",
      "epoch 776/1000   error=0.278589\n",
      "epoch 777/1000   error=0.278589\n",
      "epoch 778/1000   error=0.278590\n",
      "epoch 779/1000   error=0.278590\n",
      "epoch 780/1000   error=0.278590\n",
      "epoch 781/1000   error=0.278590\n",
      "epoch 782/1000   error=0.278590\n",
      "epoch 783/1000   error=0.278590\n",
      "epoch 784/1000   error=0.278591\n",
      "epoch 785/1000   error=0.278591\n",
      "epoch 786/1000   error=0.278591\n",
      "epoch 787/1000   error=0.278591\n",
      "epoch 788/1000   error=0.278591\n",
      "epoch 789/1000   error=0.278591\n",
      "epoch 790/1000   error=0.278592\n",
      "epoch 791/1000   error=0.278592\n",
      "epoch 792/1000   error=0.278592\n",
      "epoch 793/1000   error=0.278592\n",
      "epoch 794/1000   error=0.278592\n",
      "epoch 795/1000   error=0.278592\n",
      "epoch 796/1000   error=0.278593\n",
      "epoch 797/1000   error=0.278593\n",
      "epoch 798/1000   error=0.278593\n",
      "epoch 799/1000   error=0.278593\n",
      "epoch 800/1000   error=0.278593\n",
      "epoch 801/1000   error=0.278593\n",
      "epoch 802/1000   error=0.278594\n",
      "epoch 803/1000   error=0.278594\n",
      "epoch 804/1000   error=0.278594\n",
      "epoch 805/1000   error=0.278594\n",
      "epoch 806/1000   error=0.278594\n",
      "epoch 807/1000   error=0.278595\n",
      "epoch 808/1000   error=0.278595\n",
      "epoch 809/1000   error=0.278595\n",
      "epoch 810/1000   error=0.278595\n",
      "epoch 811/1000   error=0.278595\n",
      "epoch 812/1000   error=0.278595\n",
      "epoch 813/1000   error=0.278596\n",
      "epoch 814/1000   error=0.278596\n",
      "epoch 815/1000   error=0.278596\n",
      "epoch 816/1000   error=0.278596\n",
      "epoch 817/1000   error=0.278596\n",
      "epoch 818/1000   error=0.278597\n",
      "epoch 819/1000   error=0.278597\n",
      "epoch 820/1000   error=0.278597\n",
      "epoch 821/1000   error=0.278597\n",
      "epoch 822/1000   error=0.278597\n",
      "epoch 823/1000   error=0.278597\n",
      "epoch 824/1000   error=0.278598\n",
      "epoch 825/1000   error=0.278598\n",
      "epoch 826/1000   error=0.278598\n",
      "epoch 827/1000   error=0.278598\n",
      "epoch 828/1000   error=0.278598\n",
      "epoch 829/1000   error=0.278599\n",
      "epoch 830/1000   error=0.278599\n",
      "epoch 831/1000   error=0.278599\n",
      "epoch 832/1000   error=0.278599\n",
      "epoch 833/1000   error=0.278599\n",
      "epoch 834/1000   error=0.278600\n",
      "epoch 835/1000   error=0.278600\n",
      "epoch 836/1000   error=0.278600\n",
      "epoch 837/1000   error=0.278600\n",
      "epoch 838/1000   error=0.278600\n",
      "epoch 839/1000   error=0.278601\n",
      "epoch 840/1000   error=0.278601\n",
      "epoch 841/1000   error=0.278601\n",
      "epoch 842/1000   error=0.278601\n",
      "epoch 843/1000   error=0.278601\n",
      "epoch 844/1000   error=0.278602\n",
      "epoch 845/1000   error=0.278602\n",
      "epoch 846/1000   error=0.278602\n",
      "epoch 847/1000   error=0.278602\n",
      "epoch 848/1000   error=0.278602\n",
      "epoch 849/1000   error=0.278603\n",
      "epoch 850/1000   error=0.278603\n",
      "epoch 851/1000   error=0.278603\n",
      "epoch 852/1000   error=0.278603\n",
      "epoch 853/1000   error=0.278603\n",
      "epoch 854/1000   error=0.278604\n",
      "epoch 855/1000   error=0.278604\n",
      "epoch 856/1000   error=0.278604\n",
      "epoch 857/1000   error=0.278604\n",
      "epoch 858/1000   error=0.278604\n",
      "epoch 859/1000   error=0.278605\n",
      "epoch 860/1000   error=0.278605\n",
      "epoch 861/1000   error=0.278605\n",
      "epoch 862/1000   error=0.278605\n",
      "epoch 863/1000   error=0.278605\n",
      "epoch 864/1000   error=0.278606\n",
      "epoch 865/1000   error=0.278606\n",
      "epoch 866/1000   error=0.278606\n",
      "epoch 867/1000   error=0.278606\n",
      "epoch 868/1000   error=0.278606\n",
      "epoch 869/1000   error=0.278607\n",
      "epoch 870/1000   error=0.278607\n",
      "epoch 871/1000   error=0.278607\n",
      "epoch 872/1000   error=0.278607\n",
      "epoch 873/1000   error=0.278607\n",
      "epoch 874/1000   error=0.278608\n",
      "epoch 875/1000   error=0.278608\n",
      "epoch 876/1000   error=0.278608\n",
      "epoch 877/1000   error=0.278608\n",
      "epoch 878/1000   error=0.278608\n",
      "epoch 879/1000   error=0.278609\n",
      "epoch 880/1000   error=0.278609\n",
      "epoch 881/1000   error=0.278609\n",
      "epoch 882/1000   error=0.278609\n",
      "epoch 883/1000   error=0.278610\n",
      "epoch 884/1000   error=0.278610\n",
      "epoch 885/1000   error=0.278610\n",
      "epoch 886/1000   error=0.278610\n",
      "epoch 887/1000   error=0.278610\n",
      "epoch 888/1000   error=0.278611\n",
      "epoch 889/1000   error=0.278611\n",
      "epoch 890/1000   error=0.278611\n",
      "epoch 891/1000   error=0.278611\n",
      "epoch 892/1000   error=0.278611\n",
      "epoch 893/1000   error=0.278612\n",
      "epoch 894/1000   error=0.278612\n",
      "epoch 895/1000   error=0.278612\n",
      "epoch 896/1000   error=0.278612\n",
      "epoch 897/1000   error=0.278612\n",
      "epoch 898/1000   error=0.278613\n",
      "epoch 899/1000   error=0.278613\n",
      "epoch 900/1000   error=0.278613\n",
      "epoch 901/1000   error=0.278613\n",
      "epoch 902/1000   error=0.278614\n",
      "epoch 903/1000   error=0.278614\n",
      "epoch 904/1000   error=0.278614\n",
      "epoch 905/1000   error=0.278614\n",
      "epoch 906/1000   error=0.278614\n",
      "epoch 907/1000   error=0.278615\n",
      "epoch 908/1000   error=0.278615\n",
      "epoch 909/1000   error=0.278615\n",
      "epoch 910/1000   error=0.278615\n",
      "epoch 911/1000   error=0.278615\n",
      "epoch 912/1000   error=0.278616\n",
      "epoch 913/1000   error=0.278616\n",
      "epoch 914/1000   error=0.278616\n",
      "epoch 915/1000   error=0.278616\n",
      "epoch 916/1000   error=0.278616\n",
      "epoch 917/1000   error=0.278617\n",
      "epoch 918/1000   error=0.278617\n",
      "epoch 919/1000   error=0.278617\n",
      "epoch 920/1000   error=0.278617\n",
      "epoch 921/1000   error=0.278618\n",
      "epoch 922/1000   error=0.278618\n",
      "epoch 923/1000   error=0.278618\n",
      "epoch 924/1000   error=0.278618\n",
      "epoch 925/1000   error=0.278618\n",
      "epoch 926/1000   error=0.278619\n",
      "epoch 927/1000   error=0.278619\n",
      "epoch 928/1000   error=0.278619\n",
      "epoch 929/1000   error=0.278619\n",
      "epoch 930/1000   error=0.278619\n",
      "epoch 931/1000   error=0.278620\n",
      "epoch 932/1000   error=0.278620\n",
      "epoch 933/1000   error=0.278620\n",
      "epoch 934/1000   error=0.278620\n",
      "epoch 935/1000   error=0.278621\n",
      "epoch 936/1000   error=0.278621\n",
      "epoch 937/1000   error=0.278621\n",
      "epoch 938/1000   error=0.278621\n",
      "epoch 939/1000   error=0.278621\n",
      "epoch 940/1000   error=0.278622\n",
      "epoch 941/1000   error=0.278622\n",
      "epoch 942/1000   error=0.278622\n",
      "epoch 943/1000   error=0.278622\n",
      "epoch 944/1000   error=0.278623\n",
      "epoch 945/1000   error=0.278623\n",
      "epoch 946/1000   error=0.278623\n",
      "epoch 947/1000   error=0.278623\n",
      "epoch 948/1000   error=0.278623\n",
      "epoch 949/1000   error=0.278624\n",
      "epoch 950/1000   error=0.278624\n",
      "epoch 951/1000   error=0.278624\n",
      "epoch 952/1000   error=0.278624\n",
      "epoch 953/1000   error=0.278624\n",
      "epoch 954/1000   error=0.278625\n",
      "epoch 955/1000   error=0.278625\n",
      "epoch 956/1000   error=0.278625\n",
      "epoch 957/1000   error=0.278625\n",
      "epoch 958/1000   error=0.278626\n",
      "epoch 959/1000   error=0.278626\n",
      "epoch 960/1000   error=0.278626\n",
      "epoch 961/1000   error=0.278626\n",
      "epoch 962/1000   error=0.278626\n",
      "epoch 963/1000   error=0.278627\n",
      "epoch 964/1000   error=0.278627\n",
      "epoch 965/1000   error=0.278627\n",
      "epoch 966/1000   error=0.278627\n",
      "epoch 967/1000   error=0.278627\n",
      "epoch 968/1000   error=0.278628\n",
      "epoch 969/1000   error=0.278628\n",
      "epoch 970/1000   error=0.278628\n",
      "epoch 971/1000   error=0.278628\n",
      "epoch 972/1000   error=0.278629\n",
      "epoch 973/1000   error=0.278629\n",
      "epoch 974/1000   error=0.278629\n",
      "epoch 975/1000   error=0.278629\n",
      "epoch 976/1000   error=0.278629\n",
      "epoch 977/1000   error=0.278630\n",
      "epoch 978/1000   error=0.278630\n",
      "epoch 979/1000   error=0.278630\n",
      "epoch 980/1000   error=0.278630\n",
      "epoch 981/1000   error=0.278630\n",
      "epoch 982/1000   error=0.278631\n",
      "epoch 983/1000   error=0.278631\n",
      "epoch 984/1000   error=0.278631\n",
      "epoch 985/1000   error=0.278631\n",
      "epoch 986/1000   error=0.278632\n",
      "epoch 987/1000   error=0.278632\n",
      "epoch 988/1000   error=0.278632\n",
      "epoch 989/1000   error=0.278632\n",
      "epoch 990/1000   error=0.278632\n",
      "epoch 991/1000   error=0.278633\n",
      "epoch 992/1000   error=0.278633\n",
      "epoch 993/1000   error=0.278633\n",
      "epoch 994/1000   error=0.278633\n",
      "epoch 995/1000   error=0.278634\n",
      "epoch 996/1000   error=0.278634\n",
      "epoch 997/1000   error=0.278634\n",
      "epoch 998/1000   error=0.278634\n",
      "epoch 999/1000   error=0.278634\n",
      "epoch 1000/1000   error=0.278635\n",
      "[array([[0.52196151]]), array([[0.52447162]]), array([[0.51699462]]), array([[0.51952678]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from network import Network\n",
    "#from fc_layer import FCLayer\n",
    "#from activation_layer import ActivationLayer\n",
    "#from activations import tanh, tanh_prime\n",
    "#from losses import mse, mse_prime\n",
    "\n",
    "# training data\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# network\n",
    "net = Network()\n",
    "net.add(FCLayer(2, 3))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(3, 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# test\n",
    "out = net.predict(x_train)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve MNIST\n",
    "We didn’t implemented the Convolutional Layer but this is not a problem. \n",
    "All we need to do is to reshape our data so that it can fit into a Fully Connected Layer.\n",
    "MNIST Dataset consists of images of digits from 0 to 9, of shape 28x28x1. \n",
    "The goal is to predict what digit is drawn on a picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.247857\n",
      "epoch 2/35   error=0.116482\n",
      "epoch 3/35   error=0.094183\n",
      "epoch 4/35   error=0.080750\n",
      "epoch 5/35   error=0.069387\n",
      "epoch 6/35   error=0.059157\n",
      "epoch 7/35   error=0.050734\n",
      "epoch 8/35   error=0.044025\n",
      "epoch 9/35   error=0.038943\n",
      "epoch 10/35   error=0.034187\n",
      "epoch 11/35   error=0.030527\n",
      "epoch 12/35   error=0.027398\n",
      "epoch 13/35   error=0.025067\n",
      "epoch 14/35   error=0.022808\n",
      "epoch 15/35   error=0.021122\n",
      "epoch 16/35   error=0.019502\n",
      "epoch 17/35   error=0.018085\n",
      "epoch 18/35   error=0.016745\n",
      "epoch 19/35   error=0.015536\n",
      "epoch 20/35   error=0.014561\n",
      "epoch 21/35   error=0.013665\n",
      "epoch 22/35   error=0.012926\n",
      "epoch 23/35   error=0.012193\n",
      "epoch 24/35   error=0.011492\n",
      "epoch 25/35   error=0.010931\n",
      "epoch 26/35   error=0.010486\n",
      "epoch 27/35   error=0.010071\n",
      "epoch 28/35   error=0.009697\n",
      "epoch 29/35   error=0.009347\n",
      "epoch 30/35   error=0.009079\n",
      "epoch 31/35   error=0.008784\n",
      "epoch 32/35   error=0.008544\n",
      "epoch 33/35   error=0.008240\n",
      "epoch 34/35   error=0.008085\n",
      "epoch 35/35   error=0.007904\n",
      "\n",
      "\n",
      "predicted values : \n",
      "[array([[-0.00738892, -0.01338442, -0.10855215,  0.07209034, -0.00864237,\n",
      "        -0.00883166,  0.01155299,  0.97751188, -0.01995722, -0.04202588]]), array([[-0.01169562, -0.07322404,  0.94570175,  0.03710101,  0.03991233,\n",
      "         0.39934266,  0.1389434 , -0.01187489, -0.14012527, -0.01600758]]), array([[ 0.06452295,  0.97995063,  0.37554067, -0.09202685, -0.22711209,\n",
      "         0.24835992, -0.28253565, -0.06331221, -0.07999502, -0.29553442]])]\n",
      "true values : \n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from network import Network\n",
    "#from fc_layer import FCLayer\n",
    "#from activation_layer import ActivationLayer\n",
    "#from activations import tanh, tanh_prime\n",
    "#from losses import mse, mse_prime\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:3])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_prime(x):\n",
    "    return np.exp(-x) / (1 + np.exp(-x))**2\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "def relu_prime(x):\n",
    "    return np.array(x >= 0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Sigmoid Activation on (1.0) gives 0.7\n",
      "Applying Sigmoid Activation on (-10.0) gives 0.0\n",
      "Applying Sigmoid Activation on (0.0) gives 0.5\n",
      "Applying Sigmoid Activation on (15.0) gives 1.0\n",
      "Applying Sigmoid Activation on (-2.0) gives 0.1\n"
     ]
    }
   ],
   "source": [
    "x = 1.0\n",
    "print('Applying Sigmoid Activation on (%.1f) gives %.1f' % (x, sigmoid(x)))\n",
    "\n",
    "x = -10.0\n",
    "print('Applying Sigmoid Activation on (%.1f) gives %.1f' % (x, sigmoid(x)))\n",
    "\n",
    "x = 0.0\n",
    "print('Applying Sigmoid Activation on (%.1f) gives %.1f' % (x, sigmoid(x)))\n",
    "\n",
    "x = 15.0\n",
    "print('Applying Sigmoid Activation on (%.1f) gives %.1f' % (x, sigmoid(x)))\n",
    "\n",
    "x = -2.0\n",
    "print('Applying Sigmoid Activation on (%.1f) gives %.1f' % (x, sigmoid(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Relu on (1.0) gives 1.0\n",
      "Applying Relu on (-10.0) gives 0.0\n",
      "Applying Relu on (0.0) gives 0.0\n",
      "Applying Relu on (15.0) gives 15.0\n",
      "Applying Relu on (-20.0) gives 0.0\n"
     ]
    }
   ],
   "source": [
    "x = 1.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = -10.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = 0.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = 15.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = -20.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [1, 5, 7, 8]\n",
    "y_true = [6, 5, 2, 3]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.91       980\n",
      "           1       0.96      0.96      0.96      1135\n",
      "           2       0.87      0.76      0.81      1032\n",
      "           3       0.86      0.70      0.77      1010\n",
      "           4       0.79      0.72      0.75       982\n",
      "           5       0.54      0.74      0.62       892\n",
      "           6       0.87      0.78      0.82       958\n",
      "           7       0.82      0.87      0.84      1028\n",
      "           8       0.66      0.74      0.69       974\n",
      "           9       0.70      0.74      0.72      1009\n",
      "\n",
      "    accuracy                           0.79     10000\n",
      "   macro avg       0.80      0.79      0.79     10000\n",
      "weighted avg       0.80      0.79      0.79     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate(x_t, y_t):\n",
    "  true = [np.argmax(y) for x, y in zip(x_t, y_t)]\n",
    "  pred = [np.argmax(net.predict(x)[0]) for x, y in zip(x_t, y_t)]\n",
    "  print(classification_report(true, pred))\n",
    "evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "myDataSet = pd.read_csv(r\"C:\\Users\\User\\Desktop\\diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myDataSet.var().round(3)\n",
    "myDataSet.rename(columns={'Diabete': 'Pregnancies'}, inplace=True)\n",
    "\n",
    "x = myDataSet.drop(columns='Pregnancies')\n",
    "y = myDataSet['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.231594\n",
      "epoch 2/35   error=0.229309\n",
      "epoch 3/35   error=0.227885\n",
      "epoch 4/35   error=0.226556\n",
      "epoch 5/35   error=0.225272\n",
      "epoch 6/35   error=0.224004\n",
      "epoch 7/35   error=0.222739\n",
      "epoch 8/35   error=0.221465\n",
      "epoch 9/35   error=0.220170\n",
      "epoch 10/35   error=0.218829\n",
      "epoch 11/35   error=0.217415\n",
      "epoch 12/35   error=0.215897\n",
      "epoch 13/35   error=0.214241\n",
      "epoch 14/35   error=0.212408\n",
      "epoch 15/35   error=0.210356\n",
      "epoch 16/35   error=0.208049\n",
      "epoch 17/35   error=0.205461\n",
      "epoch 18/35   error=0.202594\n",
      "epoch 19/35   error=0.199498\n",
      "epoch 20/35   error=0.196283\n",
      "epoch 21/35   error=0.193113\n",
      "epoch 22/35   error=0.190164\n",
      "epoch 23/35   error=0.187568\n",
      "epoch 24/35   error=0.185381\n",
      "epoch 25/35   error=0.183588\n",
      "epoch 26/35   error=0.182134\n",
      "epoch 27/35   error=0.180949\n",
      "epoch 28/35   error=0.179970\n",
      "epoch 29/35   error=0.179143\n",
      "epoch 30/35   error=0.178431\n",
      "epoch 31/35   error=0.177805\n",
      "epoch 32/35   error=0.177248\n",
      "epoch 33/35   error=0.176747\n",
      "epoch 34/35   error=0.176291\n",
      "epoch 35/35   error=0.175875\n",
      "\n",
      "\n",
      "predicted values : \n",
      "[array([[0.68969298, 0.31214395]]), array([[0.3190477 , 0.68552374]]), array([[0.75861907, 0.24080432]])]\n",
      "true values : \n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from network import Network\n",
    "#from fc_layer import FCLayer\n",
    "#from activation_layer import ActivationLayer\n",
    "#from activations import tanh, tanh_prime\n",
    "#from losses import mse, mse_prime\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load MNIST from server\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "x_train.var().round(3)\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.values.reshape(x_train.shape[0], 1, 8)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.values.reshape(x_test.shape[0],1,8)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(8, 100))                \n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "net.add(FCLayer(100, 50))                   \n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "net.add(FCLayer(50, 2))                   \n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:3])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.65      0.72       100\n",
      "           1       0.52      0.70      0.60        54\n",
      "\n",
      "    accuracy                           0.67       154\n",
      "   macro avg       0.66      0.68      0.66       154\n",
      "weighted avg       0.70      0.67      0.68       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
